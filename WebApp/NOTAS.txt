NOTAS

2millones de registros x dia


JSON de capa 1 a capa 2


Server 1
Kafka, zoo, hue, hive, Hadoop

kafka y zoo levantan automaticamente
Loguear ==> User Hadoop
jps para verificar (muchos cosos java tienen q aparecer)
start-all.sh ===> levantar hadoop y yarn
./start_hive.sh ===> levantar hive
jps ==> verificar todo (JAVA)

Procesos captura datos (KAFKA) q mandan los datos al Backend de la WebApp
scripts
  - GetData (directorio) SparkKafkaGenerateJson_V1.py script: consumidor q captura data y hace las agregaciones
    - OUTPUT: JSON A MEMORIA 
    - --parking PuntaCarretasShopping
  - GetDataToHadoop (SparkStruckStreamToHadoop.py) ==> cada 5min se conecta al topico, lee el stream y lo manda a hadoop
  - Script machine learning (ml_generate_json.py)==> JSON A MEMORIA AGREGA ML ==> esta croneado
    - ML se corre el primero del mes (preparar el dato, linear regresion)
    - ML usar el modelo (dia a dia)
  - JSONToBackend: Lee el JSON y lo publica al backend 




Server 2
Generador y simulador de datos

