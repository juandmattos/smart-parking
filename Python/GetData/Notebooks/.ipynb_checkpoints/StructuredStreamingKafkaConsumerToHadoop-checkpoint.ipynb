{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import findspark\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.functions import col,year,month,dayofmonth,coalesce,lit,from_json, hour\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init(spark_home='/opt/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics = subprocess.check_output(\"/opt/kafka/bin/kafka-topics.sh --list --bootstrap-server hadoop-namenode:9092\", shell=True)\n",
    "# topics = topics.split()\n",
    "# topics = [ topic.decode(\"UTF-8\") for topic in topics ]\n",
    "# topics.pop()\n",
    "# topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session & context\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local[*]')\n",
    "         .appName('Parkings')\n",
    "         .config('spark.jars', 'file:///opt/smart-parking/Python/GetData/spark-sql-kafka-0-10_2.12-3.2.1.jar,file:///opt/smart-parking/Python/GetData/kafka-clients-3.1.0.jar')\n",
    "         .config('spark.executor.extraClassPath','file:///opt/smart-parking/Python/GetData/spark-sql-kafka-0-10_2.12-3.2.1.jar:file:///opt/smart-parking/Python/GetData/kafka-clients-3.1.0.jar')\n",
    "         .config('spark.executor.extraLibrary','file:///opt/smart-parking/Python/GetData/spark-sql-kafka-0-10_2.12-3.2.1.jar:file:///opt/smart-parking/Python/GetData/kafka-clients-3.1.0.jar')\n",
    "         .config('spark.driver.extraClassPath', 'file:///opt/smart-parking/Python/GetData/spark-sql-kafka-0-10_2.12-3.2.1.jar:file:///opt/smart-parking/Python/GetData/kafka-clients-3.1.0.jar')\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscribe to 1 topic\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"hadoop-namenode:9092\") \\\n",
    "  .option(\"subscribe\", \"XBEESmartParkingModel\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountOccupation(lista):\n",
    "    i=0\n",
    "    for opp in lista:\n",
    "        if opp == True:\n",
    "            i+=1\n",
    "    return i\n",
    "\n",
    "countOcuppation = udf(lambda x: CountOccupation(x), IntegerType())      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentagetoString(sp_total, sp_ocupied):\n",
    "    percnt = int((sp_ocupied*100)/sp_total)\n",
    "    #print(percnt)\n",
    "    if percnt > 0 and percnt <=25:\n",
    "        return \"Empty\"\n",
    "    elif percnt > 25 and percnt <=50:\n",
    "        return \"Almost Empty\"\n",
    "    elif percnt > 50 and percnt <=75:\n",
    "        return \"Almost Full\"\n",
    "    elif percnt > 75 and percnt <=100:\n",
    "        return \"Full\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "    \n",
    "percentageOcuppation = udf(lambda x, y: percentagetoString(x,y), StringType())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @pandas_udf(\"string\", PandasUDFType.GROUPED_AGG)\n",
    "# def merge_udf(v):\n",
    "#     return \",\".join(v)\n",
    "\n",
    "# @pandas_udf(\"string\", PandasUDFType.GROUPED_AGG)\n",
    "# def merge_udf_total(v):\n",
    "#     return \"|\".join(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(batch_df, batch_id):\n",
    "    batch_df.persist()\n",
    "    \n",
    "    df = batch_df.withColumn(\"value\", col(\"value\").cast(\"string\"))\n",
    "    tableSchema = StructType() \\\n",
    "            .add(\"parking_name\", StringType())\\\n",
    "            .add(\"parking_address\", StringType())\\\n",
    "            .add(\"parking_description\", StringType())\\\n",
    "            .add(\"device_timestamp\", TimestampType())\\\n",
    "            .add(\"device_address\", StringType())\\\n",
    "            .add(\"parking_latitude\", DoubleType())\\\n",
    "            .add(\"parking_longitude\", DoubleType())\\\n",
    "            .add(\"parking_temperature\", StringType())\\\n",
    "            .add(\"parking_humidity\", StringType())\\\n",
    "            .add(\"parking_uuid\", StringType())\\\n",
    "            .add(\"parking_id\", StringType())\\\n",
    "            .add(\"level_id\", StringType())\\\n",
    "            .add(\"area_id\", StringType())\\\n",
    "            .add(\"area_name\", StringType())\\\n",
    "            .add(\"spots\", IntegerType())\\\n",
    "            .add(\"slots\", ArrayType(BooleanType()))\n",
    "    \n",
    "    prov = df.select(\"*\",from_json(\"value\",tableSchema).alias(\"data_parsed\")).select(\"data_parsed.*\")\n",
    "    length = len(prov.head()[\"slots\"])\n",
    "    dlist = prov.columns\n",
    "    \n",
    "    dfalldata = prov.select(dlist)\n",
    "    \n",
    "    #dfalldata = dfalldata.orderBy(dfalldata.device_timestamp.desc())\n",
    "    dfalldata = dfalldata.dropDuplicates((['device_address']))\n",
    "    dfalldata = dfalldata.withColumn(\"area_occupied_slots\", countOcuppation(col(\"slots\")))\\\n",
    "                         .withColumn(\"area_occupation\", percentageOcuppation(col(\"spots\"), col(\"area_occupied_slots\")))\n",
    "    windowPartitionAgg  = Window.partitionBy(\"level_id\")              \n",
    "    df2 = dfalldata.withColumn(\"level_total_spots\", sum(col(\"spots\")).over(windowPartitionAgg))\\\n",
    "                   .withColumn(\"level_occupied_slots\", sum(col(\"area_occupied_slots\")).over(windowPartitionAgg))\n",
    "    \n",
    "    \"\"\"\n",
    "    A nivel del streaming como se está usando el foreachBatch hace que hagarre fragmentos de información,\n",
    "    en mi caso puntual este desarrollo serí para realizar la integración con Hadoop y no con el Backend\n",
    "    \n",
    "    en esta seccion ver parte del notebook dedicada a codigo de agregación\n",
    "    \n",
    "    \"\"\"\n",
    " \n",
    "    batch_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicia la consulta e imprime el resultado\n",
    "CHECKPOINT_DIRECTORY = 'file:///opt/smart-parking/Python/GetData/CommitLog'\n",
    "df \\\n",
    ".writeStream\\\n",
    ".trigger(processingTime='5 seconds')\\\n",
    ".outputMode(\"append\") \\\n",
    ".option(\"checkpointLocation\", CHECKPOINT_DIRECTORY)\\\n",
    ".foreachBatch(func) \\\n",
    ".start()\\\n",
    ".awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codigo para agregar en el otro notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#        \n",
    "# .withColumn(\"level_occupation\", percentageOcuppation(col(\"level_total_spots\"), col(\"level_occupied_slots\")))\\\n",
    "# .withColumn('area_name', merge_udf(dfalldata['area_name']).over(windowPartitionAgg))\\\n",
    "# .withColumn('device_address', merge_udf(dfalldata['device_address']).over(windowPartitionAgg))\\\n",
    "# .withColumn('area_occupation', merge_udf(dfalldata['area_occupation']).over(windowPartitionAgg))\\\n",
    "# .withColumn('spots', merge_udf(dfalldata['spots'].cast(\"string\")).over(windowPartitionAgg))\\\n",
    "# .withColumn('slots', merge_udf(dfalldata['slots'].cast(\"string\")).over(windowPartitionAgg))\\\n",
    "# .withColumn('area_occupied_slots', merge_udf(dfalldata['area_occupied_slots'].cast(\"string\")).over(windowPartitionAgg))\\\n",
    "# .dropDuplicates((['level_id']))\n",
    "\n",
    "# windowPartitionAgg  = Window.partitionBy(\"parking_name\")\n",
    "# df3 = df2.withColumn(\"level_total_spots\", merge_udf_total(df2['level_total_spots'].cast(\"string\")).over(windowPartitionAgg))\\\n",
    "# .withColumn(\"level_occupied_slots\", merge_udf_total(df2['level_occupied_slots'].cast(\"string\")).over(windowPartitionAgg))\\\n",
    "# .withColumn(\"level_occupation\", merge_udf_total(df2['level_occupation']).over(windowPartitionAgg))\\\n",
    "# .withColumn('area_name', merge_udf_total(df2['area_name']).over(windowPartitionAgg))\\\n",
    "# .withColumn('device_address', merge_udf_total(df2['device_address']).over(windowPartitionAgg))\\\n",
    "# .withColumn('area_occupation', merge_udf_total(df2['area_occupation']).over(windowPartitionAgg))\\\n",
    "# .withColumn('spots', merge_udf_total(df2['spots'].cast(\"string\")).over(windowPartitionAgg))\\\n",
    "# .withColumn('slots', merge_udf_total(df2['slots'].cast(\"string\")).over(windowPartitionAgg))\\\n",
    "# .withColumn('area_occupied_slots', merge_udf_total(df2['area_occupied_slots'].cast(\"string\")).over(windowPartitionAgg))\\\n",
    "# .dropDuplicates((['parking_name']))\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\\\n",
    "    .writeStream\\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_DIRECTORY)\\\n",
    "    .foreachBatch(func) \\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
